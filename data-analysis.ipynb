{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from main import tree_grow, tree_grow_b, tree_pred, tree_pred_b\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from mlxtend.evaluate import mcnemar\n",
    "from mlxtend.evaluate import mcnemar_table\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"eclipse-metrics-packages-2.0.csv\", delimiter=\";\")\n",
    "test = pd.read_csv(\"eclipse-metrics-packages-3.0.csv\", delimiter=\";\")\n",
    "list(train.columns[4:44])\n",
    "feats = ['pre',\n",
    " 'ACD_avg',\n",
    " 'ACD_max',\n",
    " 'ACD_sum',\n",
    " 'FOUT_avg',\n",
    " 'FOUT_max',\n",
    " 'FOUT_sum',\n",
    " 'MLOC_avg',\n",
    " 'MLOC_max',\n",
    " 'MLOC_sum',\n",
    " 'NBD_avg',\n",
    " 'NBD_max',\n",
    " 'NBD_sum',\n",
    " 'NOCU',\n",
    " 'NOF_avg',\n",
    " 'NOF_max',\n",
    " 'NOF_sum',\n",
    " 'NOI_avg',\n",
    " 'NOI_max',\n",
    " 'NOI_sum',\n",
    " 'NOM_avg',\n",
    " 'NOM_max',\n",
    " 'NOM_sum',\n",
    " 'NOT_avg',\n",
    " 'NOT_max',\n",
    " 'NOT_sum',\n",
    " 'NSF_avg',\n",
    " 'NSF_max',\n",
    " 'NSF_sum',\n",
    " 'NSM_avg',\n",
    " 'NSM_max',\n",
    " 'NSM_sum',\n",
    " 'PAR_avg',\n",
    " 'PAR_max',\n",
    " 'PAR_sum',\n",
    " 'TLOC_avg',\n",
    " 'TLOC_max',\n",
    " 'TLOC_sum',\n",
    " 'VG_avg',\n",
    " 'VG_max',\n",
    " 'VG_sum']\n",
    "train_x = train[feats]\n",
    "train_y = train[\"post\"]\n",
    "test_x = test[feats]\n",
    "test_y = test[\"post\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_np = train_x.to_numpy()\n",
    "test_x_np = test_x.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y_np = train_y.to_numpy()\n",
    "train_y_np = np.where(train_y_np > 0, 1, 0)\n",
    "test_y_np = test_y.to_numpy()\n",
    "test_y_np = np.where(test_y_np > 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_model = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training single tree took 1.0645403861999512 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "single_tree = tree_grow(train_x_np, train_y_np, 15, 5, 41)\n",
    "end = time()\n",
    "print(f\"Training single tree took {end-start} seconds\")\n",
    "test_y_np_pred_st = tree_pred(test_x_np, single_tree)\n",
    "preds_model[\"single tree\"] = test_y_np_pred_st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(single_tree.y) == (len(single_tree.l.y) + len(single_tree.r.y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_scores(y_true,y_pred):\n",
    "    conf_mat = confusion_matrix(y_true,y_pred)\n",
    "    tn = conf_mat[0][0]\n",
    "    fp = conf_mat[0][1]\n",
    "    fn = conf_mat[1][0]\n",
    "    tp = conf_mat[1][1]\n",
    "    print(\"Confusion matrix:\")\n",
    "    display(Markdown(f'''| True/Pred |Pos|Neg|\n",
    "|-----------|---|---|\n",
    "|       Pos |{tp}|{fn}|\n",
    "|       Neg |{fp}|{tn}|\n",
    "    '''))\n",
    "    print(\"---------------------------------\")\n",
    "    print(\"LaTex formatting:\", '''\n",
    "\\\\begin{table}[]\n",
    "\\\\begin{tabular}{|r|l|l|}\n",
    "\\\\hline\n",
    "True/Pred & Pos & Neg \\\\\\ \\hline''')\n",
    "    print(f\"Pos                             &  {tp}   &  {fn}   \\\\\\ \\hline\")\n",
    "    print(f\"Neg                             & {fp}    &  {tn}   \\\\\\ \\hline\")\n",
    "    print('''\\end{tabular}\n",
    "\\end{table}''')\n",
    "    print(\"------------------------------------\")\n",
    "    print(\"accuracy:\", accuracy_score(y_true,y_pred))\n",
    "    print(\"precision:\", precision_score(y_true,y_pred))\n",
    "    print(\"recall:\", recall_score(y_true,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| True/Pred |Pos|Neg|\n",
       "|-----------|---|---|\n",
       "|       Pos |185|128|\n",
       "|       Neg |82|266|\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "LaTex formatting: \n",
      "\\begin{table}[]\n",
      "\\begin{tabular}{|r|l|l|}\n",
      "\\hline\n",
      "True/Pred & Pos & Neg \\\\ \\hline\n",
      "Pos                             &  185   &  128   \\\\ \\hline\n",
      "Neg                             & 82    &  266   \\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "------------------------------------\n",
      "accuracy: 0.6822995461422088\n",
      "precision: 0.6928838951310862\n",
      "recall: 0.5910543130990416\n"
     ]
    }
   ],
   "source": [
    "report_scores(test_y_np, test_y_np_pred_st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root values: split_feature = pre, split_value = 4.5, y_values_1_ratio = 0.5039787798408488\n"
     ]
    }
   ],
   "source": [
    "# Print the first two-layers of the tree\n",
    "# Root node information\n",
    "root = single_tree\n",
    "split_feature = feats[root.f]\n",
    "split_value = root.s\n",
    "y_values = root.y\n",
    "y_values_1_ratio = sum(y_values)/len(y_values)\n",
    "print(f\"Root values: split_feature = {split_feature}, split_value = {split_value}, y_values_1_ratio = {y_values_1_ratio}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[187 190]\n",
      "4.5\n",
      "pre\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(single_tree.y))\n",
    "# print(single_tree.f)\n",
    "print(single_tree.s)\n",
    "print(feats[single_tree.f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[156  58]\n",
      "26.5\n",
      "VG_max\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(single_tree.l.y))\n",
    "print(single_tree.l.s)\n",
    "print(feats[single_tree.l.f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 31 132]\n",
      "0.1583333333333333\n",
      "NOI_avg\n"
     ]
    }
   ],
   "source": [
    "print(np.bincount(single_tree.r.y))\n",
    "print(single_tree.r.s)\n",
    "print(feats[single_tree.r.f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training bagging tree took 72.33398222923279 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "bagging_tree = tree_grow_b(train_x_np, train_y_np, 15, 5, 41, 100)\n",
    "end = time()\n",
    "print(f\"Training bagging tree took {end-start} seconds\")\n",
    "test_y_np_pred_bt = tree_pred_b(test_x_np, bagging_tree)\n",
    "preds_model[\"bagging tree\"] = test_y_np_pred_bt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| True/Pred |Pos|Neg|\n",
       "|-----------|---|---|\n",
       "|       Pos |207|106|\n",
       "|       Neg |39|309|\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "LaTex formatting: \n",
      "\\begin{table}[]\n",
      "\\begin{tabular}{|r|l|l|}\n",
      "\\hline\n",
      "True/Pred & Pos & Neg \\\\ \\hline\n",
      "Pos                             &  207   &  106   \\\\ \\hline\n",
      "Neg                             & 39    &  309   \\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "------------------------------------\n",
      "accuracy: 0.7806354009077155\n",
      "precision: 0.8414634146341463\n",
      "recall: 0.6613418530351438\n"
     ]
    }
   ],
   "source": [
    "report_scores(test_y_np, test_y_np_pred_bt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training random forest took 12.569201946258545 seconds\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "random_tree = tree_grow_b(train_x_np, train_y_np, 15, 5, 6, 100)\n",
    "end = time()\n",
    "print(f\"Training random forest took {end-start} seconds\")\n",
    "test_y_np_pred_rt = tree_pred_b(test_x_np, random_tree)\n",
    "preds_model[\"random forest\"] = test_y_np_pred_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "| True/Pred |Pos|Neg|\n",
       "|-----------|---|---|\n",
       "|       Pos |217|96|\n",
       "|       Neg |58|290|\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "LaTex formatting: \n",
      "\\begin{table}[]\n",
      "\\begin{tabular}{|r|l|l|}\n",
      "\\hline\n",
      "True/Pred & Pos & Neg \\\\ \\hline\n",
      "Pos                             &  217   &  96   \\\\ \\hline\n",
      "Neg                             & 58    &  290   \\\\ \\hline\n",
      "\\end{tabular}\n",
      "\\end{table}\n",
      "------------------------------------\n",
      "accuracy: 0.7670196671709532\n",
      "precision: 0.7890909090909091\n",
      "recall: 0.6932907348242812\n"
     ]
    }
   ],
   "source": [
    "report_scores(test_y_np, test_y_np_pred_rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "McNemar's test single tree vs bagging tree\n",
      "chi² statistic: 32.25196850393701, p-value: 1.3541982115265086e-08\n",
      "\n",
      "McNemar's test single tree vs random forest\n",
      "chi² statistic: 25.208333333333332, p-value: 5.14593654465032e-07\n",
      "\n",
      "McNemar's test random forest vs bagging tree\n",
      "chi² statistic: 0.9846153846153847, p-value: 0.3210619922539037\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = list(preds_model.keys())\n",
    "for model1 in models:\n",
    "    for model2 in models:\n",
    "        if not(model1 == model2):\n",
    "            print(f\"McNemar's test {model1} vs {model2}\")\n",
    "            # The code in the following three lines was adopted from [1].\n",
    "            table = mcnemar_table(y_target=test_y_np, y_model1=preds_model[model1], y_model2=preds_model[model2])\n",
    "            chi2_, p = mcnemar(ary=table, corrected=True)\n",
    "            print(f\"chi² statistic: {chi2_}, p-value: {p}\\n\")\n",
    "    models.remove(model1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] T. Toledo Jr, “Statistical tests for comparing classification algorithms,” Medium, Jan. 04, 2022. [Online]. Available: https://towardsdatascience.com/statistical-tests-for-comparing-classification-algorithms-ac1804e79bb7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
